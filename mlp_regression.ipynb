{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = (100)\n",
    "alpha = 0.0001 # alpha used by L2 regularization\n",
    "batch_size = 10\n",
    "step_size = 0.0000001\n",
    "\n",
    "\n",
    "constant_model = MLPRegressor(solver=\"sgd\", batch_size=batch_size, learning_rate_init=step_size)\n",
    "adam_model = MLPRegressor(solver=\"adam\", batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_train_loop(model: sklearn.neural_network._multilayer_perceptron.MLPRegressor, n_epochs: int, train_data:np.ndarray, train_labels:np.ndarray, test_data:np.ndarray, test_labels:np.ndarray):\n",
    "    errors = []\n",
    "    losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        model.partial_fit(train_data, train_labels)\n",
    "        preds = model.predict(test_data)\n",
    "        errors.append(mean_absolute_error(test_labels, preds))\n",
    "        losses.append(model.loss_)\n",
    "\n",
    "\n",
    "    return model, errors, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, errs, loss = partial_train_loop(constant_model, 50, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## altering constant step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sizes = [1e-8, 1e-5, 0.01]\n",
    "# passing in model causes trinaing to carry over.\n",
    "# step_sizes = [1e-8, 0.1]\n",
    "plt.figure('step size',figsize=[10,7])\n",
    "plt.rc('font', size=12)\n",
    "for step in step_sizes:\n",
    "    constant_model = MLPRegressor(solver=\"sgd\",  learning_rate_init=step_size, momentum=0)\n",
    "\n",
    "    mod, errs, loss = partial_train_loop(constant_model, 100, x_train, y_train, x_test, y_test)\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(\"Constant step size\")\n",
    "    plt.plot(range(len(loss)), loss, label = f\"step:{step}\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.plot(range(len(errs)), errs, label = f\"step:{step}\")\n",
    "# plt.title(\"mae\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constant step batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adam alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sizes = [1e-8, 1e-5, 0.01, 0.1 ]\n",
    "plt.figure(figsize=[10,7])\n",
    "plt.rc('font', size=12)\n",
    "for step in step_sizes:\n",
    "    adam_model = MLPRegressor(solver=\"adam\", learning_rate_init=step)\n",
    "\n",
    "    mod, errs, loss = partial_train_loop(adam_model, 100, x_train, y_train, x_test, y_test)\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.title(\"Adam\")\n",
    "    plt.plot(range(len(loss)), loss, label = f\"step:{step}\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.plot(range(len(errs)), errs, label = f\"step:{step}\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adam beta 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1_vals = [0.1, 0.3, 0.7, 0.9, 0.99]\n",
    "plt.figure('beta1',figsize=[10,7])\n",
    "plt.rc('font', size=12)\n",
    "for bet1 in beta1_vals:\n",
    "    adam_model = MLPRegressor(solver=\"adam\", beta_1=bet1)\n",
    "\n",
    "    mod, errs, loss = partial_train_loop(adam_model, 50, x_train, y_train, x_test, y_test)\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.title(\"Adam\")\n",
    "    plt.plot(range(len(loss)), loss, label = f\"beta 1:{bet1}\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.plot(range(len(errs)), errs, label = f\"beta 1:{bet1}\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adam beta 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta2_vals = [0.1, 0.3, 0.7, 0.9, 0.99]\n",
    "plt.figure('beta2',figsize=[10,7])\n",
    "plt.rc('font', size=12)\n",
    "for bet2 in beta2_vals:\n",
    "    adam_model = MLPRegressor(solver=\"adam\", beta_2=bet2)\n",
    "\n",
    "    mod, errs, loss = partial_train_loop(adam_model, 50, x_train, y_train, x_test, y_test)\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.title(\"Adam\")\n",
    "    plt.plot(range(len(loss)), loss, label = f\"beta 2:{bet2}\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.plot(range(len(errs)), errs, label = f\"beta 2:{bet2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ideal batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_regr = DummyRegressor(strategy=\"mean\")\n",
    "dummy_regr.fit(x_train, y_train)\n",
    "dummy_mae = mean_absolute_error(y_test,dummy_regr.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [12,25,51 ] # default = 200\n",
    "# passing in model causes trinaing to carry over.\n",
    "# step_sizes = [1e-8, 0.1]\n",
    "plt.figure('Batch Size Constant Step',figsize=[10,5])\n",
    "plt.rc('font', size=12)\n",
    "for batch in batch_sizes:\n",
    "    # constant_model = MLPRegressor(solver=\"sgd\", batch_size=batch, momentum=0, learning_rate_init=0.1)\n",
    "    adam_model = MLPRegressor(solver=\"adam\", batch_size=batch)\n",
    "    # mod, errs, loss = partial_train_loop(constant_model, 100, x_train, y_train, x_test, y_test)\n",
    "    adam_mod, adam_errs, adam_loss = partial_train_loop(adam_model, 100, x_train, y_train, x_test, y_test)\n",
    "\n",
    "\n",
    "    # plt.plot(range(len(errs)), errs, label = f\"constant, batch = {batch}\")\n",
    "    plt.plot(range(len(adam_errs)), adam_errs, label = f\"adam, batch = {batch}\")\n",
    "\n",
    "# plt.plot(range(len(dummy_mae)),dummy_mae, label = \"dumm\")\n",
    "plt.axhline(y = dummy_mae, label = 'Baseline')\n",
    "plt.legend()\n",
    "plt.title(\"Varying Batch Size\")\n",
    "plt.ylabel(\"Mean Absolute Error\")\n",
    "plt.xlabel('epochs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [12, 25, 51] # default = 200\n",
    "# passing in model causes trinaing to carry over.\n",
    "# step_sizes = [1e-8, 0.1]\n",
    "plt.figure('Batch Size Constant Step',figsize=[10,5])\n",
    "plt.rc('font', size=12)\n",
    "for batch in batch_sizes:\n",
    "    constant_model = MLPRegressor(solver=\"sgd\", batch_size=batch, learning_rate_init=0.1, momentum=0)\n",
    "\n",
    "    mod, errs, loss = partial_train_loop(constant_model, 100, x_train, y_train, x_test, y_test)\n",
    "    \n",
    "\n",
    "    # plt.subplot(2, 1, 2)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.plot(range(len(errs)), errs, label = f\"batch size:{batch}\")\n",
    "plt.axhline(y = dummy_mae, label = 'Baseline')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Varying Batch Size, Constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calc mean time taken and mean mae after 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "iters = 10\n",
    "err_sum = 0\n",
    "loss_sum = 0\n",
    "curr_time = time.time_ns()\n",
    "for i in range(iters):\n",
    "    adam_model = MLPRegressor(solver=\"adam\", batch_size=51)\n",
    "    adam_mod, adam_errs, adam_loss = partial_train_loop(adam_model, 500, x_train, y_train, x_test, y_test)\n",
    "    err_sum += min(adam_errs)\n",
    "    loss_sum += min(adam_loss)\n",
    "adam_time = time.time_ns()- curr_time\n",
    "adam_err = err_sum/iters\n",
    "adam_loss_mean = loss_sum/iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_sum = 0\n",
    "# iters = 3\n",
    "curr_time = time.time_ns()\n",
    "for i in range(iters):\n",
    "    constant_model = MLPRegressor(solver=\"sgd\", batch_size=51, momentum=0, learning_rate_init=0.1)\n",
    "    mod, errs, loss = partial_train_loop(constant_model, 500, x_train, y_train, x_test, y_test)\n",
    "    err_sum += min(errs)\n",
    "    loss_sum += min(loss)\n",
    "const_time = time.time_ns()- curr_time\n",
    "const_err = err_sum/iters\n",
    "loss_mean = loss_sum/iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"adam time {adam_time/1e9}\\nconstant time {const_time/1e9}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"adam error {adam_err}\\nconstant error {const_err}\")\n",
    "print(f\"adam loss {adam_loss_mean}\\nconstant loss {loss_mean}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb3c8892de2db7fad7dda1405878965bd170cb28ec70e3c0bc8ed3e1088822dc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
