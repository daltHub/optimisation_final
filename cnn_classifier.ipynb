{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, LeakyReLU\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_adam_opt(alpha =0.001, beta1=0.9, beta2 = 0.999):\n",
    "    return tf.keras.optimizers.Adam(learning_rate=alpha, beta_1=beta1, beta_2=beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_const_opt(alpha =0.001):\n",
    "    return tf.keras.optimizers.SGD(learning_rate=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "# url = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/cifar10.npz'\n",
    "# (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data(url)\n",
    "# n=5000\n",
    "# x_train = x_train[1:n]; y_train=y_train[1:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "n=5000\n",
    "x_train = x_train[1:n]; y_train=y_train[1:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "print(\"orig x_train shape:\", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.Sequential()\n",
    "# model.add(Conv2D(16, (3,3), padding='same', input_shape=x_train.shape[1:],activation='relu'))\n",
    "# model.add(Conv2D(16, (3,3), strides=(2,2), padding='same', activation='relu'))\n",
    "# model.add(Conv2D(32, (3,3), padding='same', activation='relu'))\n",
    "# model.add(Conv2D(32, (3,3), strides=(2,2), padding='same', activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(num_classes, activation='softmax',kernel_regularizer=regularizers.l1(0.0001)))\n",
    "# model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "# model.summary()\n",
    "\n",
    "# batch_size = 128\n",
    "# epochs = 20\n",
    "# history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "# model.save(\"cifar.model\")\n",
    "# plt.subplot(211)\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'val'], loc='upper left')\n",
    "# plt.subplot(212)\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss'); plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'val'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(opt, epochs):\n",
    "    model = keras.Sequential()\n",
    "    model.add(Conv2D(16, (3,3), padding='same', input_shape=x_train.shape[1:],activation='relu'))\n",
    "    model.add(Conv2D(16, (3,3), strides=(2,2), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(32, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(32, (3,3), strides=(2,2), padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax',kernel_regularizer=regularizers.l1(0.0001)))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    # model.summary()\n",
    "\n",
    "    batch_size = 128\n",
    "    epochs = epochs\n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "    return history.history['val_accuracy'], history.history['val_loss'], history.history['accuracy'], history.history['loss'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run1 = run_experiment(make_const_opt(alpha = 0.1), 50)\n",
    "run2 = run_experiment(make_const_opt(alpha = 0.01), 50)\n",
    "run3 = run_experiment(make_const_opt(alpha = 0.001), 50)\n",
    "\n",
    "# run2 = run_experiment(make_adam_opt(beta1=0.8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(run1[0])),run1[0], label = 'alpha = 0.1')\n",
    "plt.plot(range(len(run2[0])),run2[0], label = 'alpha = 0.01')\n",
    "plt.plot(range(len(run3[0])),run3[0], label = 'alpha = 0.001')\n",
    "plt.title('CNN classification w/ constant step size')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_range = [0.001, 0.01, 0.1]\n",
    "epochs = 50\n",
    "plt.figure()\n",
    "plt.title('Altering alpha w/ Adam')\n",
    "for alph in alpha_range:\n",
    "    plt.plot(range(epochs),run_experiment(make_adam_opt(alpha=alph),epochs=epochs)[0],label = f'alpha = {alph}' )\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1_range = [0.1, 0.9, 0.99]\n",
    "epochs = 50\n",
    "plt.figure()\n",
    "plt.title('Altering beta 1 w/ Adam')\n",
    "for bet1 in beta1_range:\n",
    "    plt.plot(range(epochs),run_experiment(make_adam_opt(beta1=bet1),epochs=epochs)[0],label = f'beta1 = {bet1}' )\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta2_range = [0.1,  0.99, 0.999]\n",
    "epochs = 50\n",
    "plt.figure()\n",
    "plt.title('Altering beta 2 w/ Adam')\n",
    "for bet2 in beta2_range:\n",
    "    plt.plot(range(epochs),run_experiment(make_adam_opt(beta2=bet2),epochs=epochs)[0],label = f'beta2 = {bet2}' )\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import winsound\n",
    "# # Play Windows exit sound.\n",
    "# winsound.PlaySound(\"SystemExit\", winsound.SND_ALIAS)\n",
    "\n",
    "# # Probably play Win     dows default sound, if any is registered (because\n",
    "# # \"*\" probably isn't the registered name of any sound).\n",
    "# while True:\n",
    "#     winsound.PlaySound(\"*\", winsound.SND_ALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_val_acc1, adam_val_loss1, adam_train_acc_1, adam_train_loss_1 = run_experiment(make_adam_opt(),epochs=epochs)\n",
    "adam_val_acc2, adam_val_loss2, adam_train_acc_2, adam_train_loss_2 = run_experiment(make_adam_opt(),epochs=epochs)\n",
    "adam_val_acc3, adam_val_loss3, adam_train_acc_3, adam_train_loss_3 = run_experiment(make_adam_opt(),epochs=epochs)\n",
    "\n",
    "const_val_acc1, const_val_loss1, const_train_acc_1, const_train_loss_1 = run_experiment(make_const_opt(alpha=0.1),epochs=epochs)\n",
    "const_val_acc2, const_val_loss2, const_train_acc_2, const_train_loss_2 = run_experiment(make_const_opt(alpha=0.1),epochs=epochs)\n",
    "const_val_acc3, const_val_loss3, const_train_acc_3, const_train_loss_3 = run_experiment(make_const_opt(alpha=0.1),epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Best Values')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(range(epochs),adam_val_acc1,'g', label = f'Adam' )\n",
    "plt.plot(range(epochs),adam_val_acc2,'g' )\n",
    "plt.plot(range(epochs),adam_val_acc3,'g' )\n",
    "\n",
    "plt.plot(range(epochs),const_val_acc1,'r' ,label = f'constant step size' )\n",
    "plt.plot(range(epochs),const_val_acc2 ,'r')\n",
    "plt.plot(range(epochs),const_val_acc3,'r')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_w_batch(opt, epochs, batch):\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(Conv2D(16, (3,3), padding='same', input_shape=x_train.shape[1:],activation='relu'))\n",
    "    model.add(Conv2D(16, (3,3), strides=(2,2), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(32, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(32, (3,3), strides=(2,2), padding='same', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax',kernel_regularizer=regularizers.l1(0.0001)))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    # model.summary()\n",
    "\n",
    "    batch_size = batch\n",
    "    epochs = epochs\n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "    return history.history['val_accuracy'], history.history['val_loss'], history.history['accuracy'], history.history['loss'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [32, 128, 256]\n",
    "\n",
    "batch_adam_val_acc1, batch_adam_val_loss1, batch_adam_train_acc_1, batch_adam_train_loss_1 = run_experiment_w_batch(make_adam_opt(),epochs=epochs, batch=batch_sizes[0])\n",
    "batch_adam_val_acc2, batch_adam_val_loss2, batch_adam_train_acc_2, batch_adam_train_loss_2 = run_experiment_w_batch(make_adam_opt(),epochs=epochs, batch=batch_sizes[1])\n",
    "batch_adam_val_acc3, batch_adam_val_loss3, batch_adam_train_acc_3, batch_adam_train_loss_3 = run_experiment_w_batch(make_adam_opt(),epochs=epochs, batch=batch_sizes[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_const_val_acc1, batch_const_val_loss1, batch_const_train_acc_1, batch_const_train_loss_1 = run_experiment_w_batch(make_const_opt(alpha=0.1),epochs=epochs, batch=batch_sizes[0])\n",
    "batch_const_val_acc2, batch_const_val_loss2, batch_const_train_acc_2, batch_const_train_loss_2 = run_experiment_w_batch(make_const_opt(alpha=0.1),epochs=epochs, batch=batch_sizes[0])\n",
    "batch_const_val_acc3, batch_const_val_loss3, batch_const_train_acc_3, batch_const_train_loss_3 = run_experiment_w_batch(make_const_opt(alpha=0.1),epochs=epochs, batch=batch_sizes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Varying Batch Size')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(range(epochs),batch_adam_val_acc1, label = f'Adam : {batch_sizes[0]}' )\n",
    "plt.plot(range(epochs),batch_adam_val_acc2,label = f'Adam : {batch_sizes[1]}' )\n",
    "plt.plot(range(epochs),batch_adam_val_acc3, label = f'Adam : {batch_sizes[2]}')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Varying Batch Size')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(range(epochs),batch_const_val_acc1,label = f'Constant : {batch_sizes[0]}' )\n",
    "plt.plot(range(epochs),batch_const_val_acc2 , label = f'Constant : {batch_sizes[1]}')\n",
    "plt.plot(range(epochs),batch_const_val_acc3, label = f'Constant : {batch_sizes[2]}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get value for mean best loss, mean best accuracy and mean execution time for 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop\n",
    "import time\n",
    "curr_time = time.time_ns()\n",
    "acc_sum, loss_sum = 0,0\n",
    "for i in range(5):\n",
    "    acc, loss, train_acc, train_loss = run_experiment(make_const_opt(alpha = 0.1), 20)\n",
    "    acc_sum = acc_sum + max(acc)\n",
    "    loss_sum = loss_sum + min(loss)\n",
    "time_taken = time.time_ns() - curr_time\n",
    "acc_sum = acc_sum/5\n",
    "loss_sum = loss_sum/5\n",
    "\n",
    "curr_time = time.time_ns()\n",
    "adam_acc_sum, adam_loss_sum = 0,0\n",
    "for i in range(5):\n",
    "    adam_acc, adam_loss, adam_train_acc, adam_train_loss = run_experiment(make_adam_opt(), 20)\n",
    "    adam_acc_sum = adam_acc_sum + max(adam_acc)\n",
    "    adam_loss_sum = adam_loss_sum + min(adam_loss)\n",
    "adam_time_taken = time.time_ns() - curr_time\n",
    "adam_acc_sum = adam_acc_sum/5\n",
    "adam_loss_sum = adam_loss_sum/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loss', loss_sum,' adam_loss ',adam_loss_sum)\n",
    "print('acc', acc_sum,' adam_acc ',adam_acc_sum)\n",
    "print('time', time_taken/1e9,' adam_time ',adam_time_taken/1e9)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb3c8892de2db7fad7dda1405878965bd170cb28ec70e3c0bc8ed3e1088822dc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
